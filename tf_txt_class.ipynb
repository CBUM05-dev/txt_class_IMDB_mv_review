{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fe6d49-bab7-417f-8c11-fc9cc287c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "data = keras.datasets.imdb\n",
    "\n",
    "#\"Ne garder que les 10 000 mots les plus fréquents du dataset\".\n",
    "# we are going to leave words that are occuring one time or twice \n",
    "#not throwing them into our model\n",
    "(train_data,train_labels),(test_data,test_labels) = data.load_data(num_words=88000)\n",
    "\n",
    "print(train_data[0])\n",
    "# we see integer encoded words , each integer is a word\n",
    "# this is fine for the computer to read it , but not for us \n",
    "# we should find the maping for each word to read it\n",
    "\n",
    "# this gives us a dictionary that has those keys(integers) and those mappings(words)\n",
    "word_index = data.get_word_index()\n",
    "\n",
    "# values are deplaced in train_data with 3 to keep 0 1 2 reserved\n",
    "#to have same values in word_index and train_data , we add +3\n",
    "#padding to make all oiur reviews the same lenght\n",
    "# if a review is 100 other 200 we add a bench of padding to the end of the first to make it 200\n",
    "# and our model will ofc know that we should not look at this padding\n",
    "word_index = {k: v+3 for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0         #padding\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2       #unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "\n",
    "reverse_word_index = dict([(value , key) for (key,value) in word_index.items()])\n",
    "\n",
    "# ? → what to return if the key doesn’t exist in the dictionary.\n",
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i,\"?\") for i in text])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4413a6b-c687-41b6-ab84-2f467bb18154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n",
      "68 260\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(test_data[0]))\n",
    "print(len(test_data[0]), len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7db41f83-a5ff-49d8-9a24-20d3dd1ffa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "250 250\n"
     ]
    }
   ],
   "source": [
    "# the differnt lengths won't work for our model\n",
    "# we need to know our input shape is going to be\n",
    "# to determine how many input neurones and output neurones\n",
    "# we could pick the longest review and make all the reviews the same len\n",
    "# but we will pick an arbitrary length and make all the riviews that len\n",
    "# by adding pad <PAD> or removing words\n",
    "#params : train_data , value of what we add , padding = \"post\" : we add after not before\n",
    "\n",
    "#preprocessing data : make the data in a form tha our model can accept\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data ,\n",
    "                                        value=word_index[\"<PAD>\"],\n",
    "                                        padding = \"post\",\n",
    "                                        maxlen=250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data ,\n",
    "                                                value = word_index[\"<PAD>\"],\n",
    "                                                padding=\"post\",\n",
    "                                                maxlen=250)\n",
    "\n",
    "print(len(train_data) , len(test_data))\n",
    "print(len(test_data[0]), len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b8e5865-5078-44f9-8434-e0789faa96f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_3      │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_3      │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Epoch 1/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.5376 - loss: 0.6918 - val_accuracy: 0.6693 - val_loss: 0.6850\n",
      "Epoch 2/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.7015 - loss: 0.6804 - val_accuracy: 0.7175 - val_loss: 0.6672\n",
      "Epoch 3/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7402 - loss: 0.6569 - val_accuracy: 0.7634 - val_loss: 0.6339\n",
      "Epoch 4/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.7897 - loss: 0.6157 - val_accuracy: 0.7840 - val_loss: 0.5897\n",
      "Epoch 5/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8084 - loss: 0.5652 - val_accuracy: 0.8040 - val_loss: 0.5409\n",
      "Epoch 6/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8313 - loss: 0.5087 - val_accuracy: 0.8240 - val_loss: 0.4902\n",
      "Epoch 7/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8544 - loss: 0.4526 - val_accuracy: 0.8397 - val_loss: 0.4456\n",
      "Epoch 8/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8685 - loss: 0.4038 - val_accuracy: 0.8479 - val_loss: 0.4107\n",
      "Epoch 9/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.8774 - loss: 0.3677 - val_accuracy: 0.8513 - val_loss: 0.3882\n",
      "Epoch 10/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8834 - loss: 0.3390 - val_accuracy: 0.8618 - val_loss: 0.3621\n",
      "Epoch 11/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9028 - loss: 0.3040 - val_accuracy: 0.8655 - val_loss: 0.3455\n",
      "Epoch 12/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9021 - loss: 0.2868 - val_accuracy: 0.8720 - val_loss: 0.3341\n",
      "Epoch 13/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9159 - loss: 0.2636 - val_accuracy: 0.8739 - val_loss: 0.3209\n",
      "Epoch 14/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9219 - loss: 0.2430 - val_accuracy: 0.8734 - val_loss: 0.3157\n",
      "Epoch 15/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9276 - loss: 0.2252 - val_accuracy: 0.8793 - val_loss: 0.3046\n",
      "Epoch 16/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9277 - loss: 0.2177 - val_accuracy: 0.8835 - val_loss: 0.2985\n",
      "Epoch 17/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9402 - loss: 0.1963 - val_accuracy: 0.8829 - val_loss: 0.2934\n",
      "Epoch 18/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9410 - loss: 0.1874 - val_accuracy: 0.8846 - val_loss: 0.2890\n",
      "Epoch 19/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9459 - loss: 0.1786 - val_accuracy: 0.8863 - val_loss: 0.2852\n",
      "Epoch 20/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9489 - loss: 0.1684 - val_accuracy: 0.8856 - val_loss: 0.2845\n",
      "Epoch 21/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9521 - loss: 0.1576 - val_accuracy: 0.8888 - val_loss: 0.2801\n",
      "Epoch 22/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9582 - loss: 0.1469 - val_accuracy: 0.8882 - val_loss: 0.2801\n",
      "Epoch 23/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9611 - loss: 0.1378 - val_accuracy: 0.8900 - val_loss: 0.2776\n",
      "Epoch 24/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9672 - loss: 0.1277 - val_accuracy: 0.8898 - val_loss: 0.2780\n",
      "Epoch 25/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9674 - loss: 0.1252 - val_accuracy: 0.8906 - val_loss: 0.2768\n",
      "Epoch 26/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9682 - loss: 0.1168 - val_accuracy: 0.8786 - val_loss: 0.2947\n",
      "Epoch 27/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9708 - loss: 0.1120 - val_accuracy: 0.8906 - val_loss: 0.2752\n",
      "Epoch 28/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9750 - loss: 0.1021 - val_accuracy: 0.8730 - val_loss: 0.3012\n",
      "Epoch 29/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9708 - loss: 0.1033 - val_accuracy: 0.8894 - val_loss: 0.2790\n",
      "Epoch 30/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9788 - loss: 0.0908 - val_accuracy: 0.8832 - val_loss: 0.2874\n",
      "Epoch 31/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9794 - loss: 0.0868 - val_accuracy: 0.8891 - val_loss: 0.2810\n",
      "Epoch 32/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9796 - loss: 0.0854 - val_accuracy: 0.8891 - val_loss: 0.2816\n",
      "Epoch 33/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9817 - loss: 0.0772 - val_accuracy: 0.8896 - val_loss: 0.2822\n",
      "Epoch 34/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9826 - loss: 0.0777 - val_accuracy: 0.8912 - val_loss: 0.2807\n",
      "Epoch 35/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9843 - loss: 0.0715 - val_accuracy: 0.8897 - val_loss: 0.2825\n",
      "Epoch 36/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9850 - loss: 0.0698 - val_accuracy: 0.8889 - val_loss: 0.2877\n",
      "Epoch 37/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9863 - loss: 0.0624 - val_accuracy: 0.8912 - val_loss: 0.2853\n",
      "Epoch 38/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9895 - loss: 0.0577 - val_accuracy: 0.8907 - val_loss: 0.2883\n",
      "Epoch 39/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9891 - loss: 0.0555 - val_accuracy: 0.8896 - val_loss: 0.2889\n",
      "Epoch 40/40\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9908 - loss: 0.0528 - val_accuracy: 0.8873 - val_loss: 0.2960\n"
     ]
    }
   ],
   "source": [
    "#define the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# instead of passing a list of layers we will use model.add\n",
    "#we want our output either the review is positive or negative\n",
    "#so we will have one neurone as output to decide 0 1\n",
    "# get our model to understand our words that they have a similar meaning\n",
    "# and to kind of group those words together in a similar form or a similar way\n",
    "#we don't know which words are similar to each other\n",
    "\n",
    "\n",
    "#embeding layer : it generates word vectors for each word that we pass it\n",
    "# and try to make the related vectors : words(integers) close to each other\n",
    "# in a simple way we can say based on the word surrounding these words\n",
    "# a word vector can be in any kind of dimensional space\n",
    "# in this case we've picked 16 dimension for each word vector\n",
    "# a vector is a straight line with a bench of different coeff in a space\n",
    "model.add(keras.layers.Embedding(88000,16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16 , activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "print(\"----------------\")\n",
    "model.compile(optimizer=\"adam\" , loss=\"binary_crossentropy\" , metrics=[\"accuracy\"])\n",
    "\n",
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]\n",
    "\n",
    "fitModel = model.fit(x_train , y_train , epochs=40,\n",
    "                     batch_size=512 , validation_data=(x_val, y_val),\n",
    "                    verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b858ab2-93c9-45b2-95ab-9786068c776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8767 - loss: 0.3125\n",
      "[0.3198740482330322, 0.8753600120544434]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5d355c1-ab57-4f34-879b-d06144f40017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Review : \n",
      "<START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction : [0.11873663]\n",
      "Actual : 0\n"
     ]
    }
   ],
   "source": [
    "# shape = (250,) → une séquence de 250 mots\n",
    "test_review = test_data[0]\n",
    "# shape = (1, 250)\n",
    "#Ça ajoute une nouvelle dimension à la position 0 (le début).\n",
    "test_review_dim = np.expand_dims(test_review , axis=0 )\n",
    "predict = model.predict(test_review_dim)\n",
    "print(\"Review : \")\n",
    "print(decode_review(test_review))\n",
    "print(\"Prediction : \" + str(predict[0]))\n",
    "print(\"Actual : \" + str(test_labels[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d9f44df-b4da-4c5b-a930-f845950e9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# every time we want to make a prediction we have to retrain the model\n",
    "#in this case it's fine because it takes us only few minutes\n",
    "# but in most real cases it will take hours or days \n",
    "# so we have to save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e86d4fe4-7eea-4f98-8c27-53874218339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save a model : name_of_model.save()\n",
    "#h5 an extension for a saved model in keras and tensorflow\n",
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a7daafa-b0a2-4b48-8b43-cf0121232a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after saving the model we don't need all this stuff\n",
    "# we just have to load this in\n",
    "# we can train a bunch of different models and tweak hyperparametres of them\n",
    "#like changing the amount of neurones in the hidden layers\n",
    "#and only save the best model with higher acc\n",
    "model_saved = keras.models.load_model(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "039e6b6e-f42d-4d29-ba23-6f7fe7cd1a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "I watched this movie last night and I was truly impressed The acting was phenomenal especially the lead actor who delivered a powerful and emotional performance The storyline was engaging from start to finish with unexpected twists that kept me hooked The soundtrack matched the scenes perfectly and elevated the emotional impact I highly recommend this film to anyone looking for a meaningful and unforgettable experience\n",
      "[[    1    13   296    14    20   236   314     5    13    16   371  1555\n",
      "      4   116    16  6814   262     4   485   284    37  2132     6   976\n",
      "      5   921   239     4   769    16  1728    39   380     8  1363    19\n",
      "   2076  1299    15   828    72  3305     4   816  4722     4   139   950\n",
      "      5 11214     4   921  1488    13   545   386    14    22     8   259\n",
      "    267    18     6  3191     5  3210   585     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n",
      "[0.99693143]\n"
     ]
    }
   ],
   "source": [
    " # using with open so i don't have to close the file after\n",
    "# we need to remove ' \" , () from our text so when we split our words with \" \"\n",
    "# we get correct words not art, in example or \"The\n",
    "# The big problem often is the data you have to make it good\n",
    "\n",
    "\n",
    "def review_encode(s) :\n",
    "    encoded = [1]  # 1 refers to <START> all reviews begin with start\n",
    "    for word in s :\n",
    "        word = word.lower()\n",
    "        if word in word_index :\n",
    "            encoded.append(word_index[word])\n",
    "        else :\n",
    "            encoded.append(2)   #<UNK>\n",
    "    return encoded\n",
    "\n",
    "\n",
    "with open(\"test.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.replace(\",\", \"\").replace(\".\", \"\").replace(\")\", \"\") \\\n",
    "                   .replace(\"(\", \"\").replace(\":\", \"\").replace('\"', \"\").strip()  # 🛠️ corriger \"\\\\\" ici\n",
    "        nline = line.split(\" \")  # on sépare les mots\n",
    "        encode = review_encode(nline)  # sequence de nbr de mots\n",
    "        encode = keras.preprocessing.sequence.pad_sequences([encode],\n",
    "                        value=word_index[\"<PAD>\"],\n",
    "                        padding=\"post\",\n",
    "                        maxlen=250)\n",
    "\n",
    "        predict = model.predict(encode)\n",
    "        print(line)\n",
    "        print(encode)\n",
    "        print(predict[0])\n",
    "        \n",
    "                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967d745-875c-4419-8986-6689992554db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
